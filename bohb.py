# -*- coding: utf-8 -*-
"""BOHB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xt504GskN7q2RbsunuXEfrdbm7OXlNHh
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import random

# Define the search space for hyperparameters
search_space = {
    "learning_rate": (0.0001, 0.1),  # Continuous space (min, max)
    "num_layers": (1, 3),           # Discrete space (min, max)
    "hidden_units": (32, 256)       # Discrete space (min, max)
}

# Load MNIST dataset
def load_mnist():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    train_dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(root="./data", train=False, transform=transform, download=True)

    return train_dataset, test_dataset

train_dataset, test_dataset = load_mnist()

# Define a simple neural network with flexible architecture
class SimpleNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_units, num_layers):
        super(SimpleNN, self).__init__()
        layers = []
        in_features = input_size

        for _ in range(num_layers):
            layers.append(nn.Linear(in_features, hidden_units))
            layers.append(nn.ReLU())
            in_features = hidden_units

        layers.append(nn.Linear(in_features, output_size))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# Define the objective function
def objective_function(config, budget):
    learning_rate = config["learning_rate"]
    num_layers = config["num_layers"]
    hidden_units = config["hidden_units"]

    model = SimpleNN(28 * 28, 10, hidden_units, num_layers).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # Train the model for the given budget (number of epochs)
    model.train()
    for epoch in range(int(budget)):
        for images, labels in train_loader:
            images = images.view(-1, 28 * 28).to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

    # Evaluate the model on the test dataset
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.view(-1, 28 * 28).to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total
    return 1 - accuracy  # Minimize the error rate

# Tree-structured Parzen Estimator (TPE) implementation
class TPE:
    def __init__(self):
        self.data = []  # Stores (config, loss) pairs

    def add(self, config, loss):
        self.data.append((config, loss))

    def sample(self, search_space):
        if len(self.data) < 10:
            return self.random_sample(search_space)

        losses = np.array([loss for _, loss in self.data])
        median_loss = np.median(losses)
        promising = [config for config, loss in self.data if loss < median_loss]
        non_promising = [config for config, loss in self.data if loss >= median_loss]

        if random.random() < 0.8:
            return random.choice(promising)
        else:
            return random.choice(non_promising)

    def random_sample(self, search_space):
        return {
            "learning_rate": np.random.uniform(*search_space["learning_rate"]),
            "num_layers": np.random.randint(*search_space["num_layers"]),
            "hidden_units": np.random.randint(*search_space["hidden_units"])
        }

# Hyperband implementation
class Hyperband:
    def __init__(self, max_budget, eta=3):
        self.max_budget = max_budget
        self.eta = eta

    def run(self, objective_function, tpe, search_space, n_configs):
        results = []
        budget = self.max_budget // (self.eta**(np.log(n_configs) // np.log(self.eta)))
        while n_configs > 0:
            configs = [tpe.sample(search_space) for _ in range(n_configs)]
            performances = [
                (config, objective_function(config, budget))
                for config in configs
            ]
            performances.sort(key=lambda x: x[1])
            top_k = len(performances) // self.eta
            results.extend(performances[:top_k])
            n_configs = top_k
            budget *= self.eta
        return results

# BOHB implementation
class BOHB:
    def __init__(self, search_space, max_budget, eta=3, total_iterations=10):
        self.search_space = search_space
        self.max_budget = max_budget
        self.eta = eta
        self.total_iterations = total_iterations
        self.tpe = TPE()
        self.hyperband = Hyperband(max_budget, eta)

    def optimize(self, objective_function):
        for iteration in range(self.total_iterations):
            print(f"Iteration {iteration + 1}/{self.total_iterations}")
            results = self.hyperband.run(objective_function, self.tpe, self.search_space, n_configs=9)
            for config, loss in results:
                self.tpe.add(config, loss)
                print(f"Config: {config}, Loss: {loss:.4f}")
            print("Iteration completed.\n")

        best_config, best_loss = min(self.tpe.data, key=lambda x: x[1])
        print(f"Best Config: {best_config}, Best Loss: {best_loss:.4f}")
        return best_config, best_loss

# Run BOHB optimization
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bohb = BOHB(search_space=search_space, max_budget=9, eta=3, total_iterations=5)
best_config, best_loss = bohb.optimize(objective_function)